{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MUSE"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-01-09T10:50:15.262984Z","iopub.status.busy":"2025-01-09T10:50:15.262732Z","iopub.status.idle":"2025-01-09T10:50:25.569927Z","shell.execute_reply":"2025-01-09T10:50:25.568795Z","shell.execute_reply.started":"2025-01-09T10:50:15.262959Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lucamodica/mambaforge/envs/muse/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package punkt to /home/lucamodica/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/lucamodica/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import os\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import pandas as pd\n","import numpy as np\n","import librosa\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import classification_report\n","import joblib\n","import nltk\n","import torchaudio\n","from torch.nn.utils.rnn import pad_sequence\n","from transformers import AutoModel, AutoProcessor, AutoTokenizer, AutoFeatureExtractor\n","from tqdm import tqdm\n","from sklearn.metrics import balanced_accuracy_score\n","\n","# Ensure NLTK data is downloaded\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["text_model_name = \"bert-base-uncased\"\n","audio_model_name = \"openai/whisper-small\""]},{"cell_type":"markdown","metadata":{},"source":["## Data"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def get_root_dir(split):\n","    return f'data/{split}'"]},{"cell_type":"markdown","metadata":{},"source":["### Utils"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["def meld_collate_fn(batch):\n","    # batch is a list of conversation dicts (one per item in dataset)\n","    # We can combine them into a single batch,\n","    # but each conversation may have different # of utterances.\n","\n","    dialog_ids = []\n","    audio_lists = []\n","    text_lists = []\n","    emotion_lists = []\n","    sentiment_lists = []\n","\n","    for conv in batch:\n","        dialog_ids.append(conv[\"dialog_id\"])\n","\n","        # Convert audio_list (list of numpy arrays) to tensors and pad\n","        fbank_tensors = [torch.tensor(fbank) for fbank in conv[\"audio_list\"]]\n","        # Pad along the time dim (T)\n","        fbank_padded = pad_sequence(fbank_tensors, batch_first=True)\n","        audio_lists.append(fbank_padded)\n","\n","        text_lists.append(conv[\"text_list\"])\n","        emotion_lists.append(torch.tensor(conv[\"emotion_list\"]))\n","        sentiment_lists.append(torch.tensor(conv[\"sentiment_list\"]))\n","\n","    # Return them \"as is\", or do further padding if needed.\n","    return {\n","        \"dialog_ids\": dialog_ids,\n","        \"audio_lists\": audio_lists,\n","        \"text_lists\": text_lists,\n","        \"emotion_lists\": emotion_lists,\n","        \"sentiment_lists\": sentiment_lists\n","    }\n","\n","\n","def custom_text_preprocessor(text):\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":["### fbanks extractor"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import torchaudio\n","import torch\n","import torchaudio.transforms as T\n","\n","def wav2fbank(filename):\n","    waveform, sr = torchaudio.load(filename)\n","    waveform = waveform - waveform.mean()\n","\n","    try:\n","        fbank = torchaudio.compliance.kaldi.fbank(\n","            waveform,\n","            htk_compat=True,\n","            sample_frequency=sr,\n","            use_energy=False,\n","            window_type='hanning',\n","            num_mel_bins=128,\n","            dither=0.0,\n","            frame_shift=10\n","        )\n","    except:\n","        fbank = torch.zeros([512, 128]) + 0.01\n","        print('there is a loading error')\n","\n","    target_length = 1024\n","    n_frames = fbank.shape[0]\n","\n","    p = target_length - n_frames\n","\n","    # cut and pad\n","    if p > 0:\n","        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n","        fbank = m(fbank)\n","    elif p < 0:\n","        fbank = fbank[0:target_length, :]\n","\n","    return fbank"]},{"cell_type":"markdown","metadata":{},"source":["### apply fbanks extractor to convert audio"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# convert audio to fbank from train, dev and test\n","# for split in ['train', 'dev', 'test']:\n","#     root_dir = get_root_dir(split)\n","#     df = pd.read_csv(f'{root_dir}/{split}_sent_emo.csv')\n","#     for i, row in df.iterrows():\n","#         dia_id = row['Dialogue_ID']\n","#         utt_id = row['Utterance_ID']\n","#         audio_file = f'{root_dir}/audio/dia{dia_id}_utt{utt_id}.wav'\n","#         fbank_file = f'{root_dir}/fbank/dia{dia_id}_utt{utt_id}.npy'\n","        \n","#         # create the fbank directory if it does not exist\n","#         if not os.path.exists(f'{root_dir}/fbank/'):\n","#             os.makedirs(f'{root_dir}/fbank/')\n","        \n","#         # check if the fbank file already exists\n","#         if not os.path.exists(f'{root_dir}/fbank/dia{dia_id}_utt{utt_id}.npy'):\n","#             fbank = wav2fbank(audio_file)\n","#             fbank = fbank.numpy()\n","#             np.save(f'{root_dir}/fbank/dia{dia_id}_utt{utt_id}.npy', fbank)\n","#         else:\n","#             print(f'{fbank_file} already exists')"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset and dataloader class"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import torchaudio\n","import torchaudio.transforms as T\n","from torch.utils.data import Dataset\n","import torch\n","\n","\n","class MELDConversationDataset(Dataset):\n","    def __init__(self, csv_file, \n","        root_dir='./data',\n","        audio_processor=None,\n","        audio_transform=None,\n","        text_transform=None, \n","        target_length=1024):\n","        \"\"\"\n","        Dataset class for MELD conversations with support for torchaudio transforms.\n","\n","        :param csv_file: Path to the CSV file with conversation metadata.\n","        :param root_dir: Root directory containing audio files (default: './data').\n","        :param audio_transform: A torchaudio transform to apply to the waveform (default: None).\n","                                If None, the raw waveform is used.\n","        :param target_length: Number of time frames to pad or truncate to (default: 1024).\n","        \"\"\"\n","        df = pd.read_csv(f'{root_dir}/{csv_file}')\n","\n","        # Order the dataframe rows by Dialogue_ID and Utterance_ID\n","        df = df.sort_values(by=['Dialogue_ID', 'Utterance_ID'])\n","\n","        # Class counts for emotions and sentiments\n","        self.emotion_class_counts = {i: 0 for i in range(7)}\n","        self.sentiment_class_counts = {i: 0 for i in range(3)}\n","        self.max_dialogue_size = 0\n","\n","        self.dialogues = {}  # key: dialogue_id, value: list of utterance dicts\n","        self.audio_transform = audio_transform\n","        self.text_transform = text_transform\n","        self.target_length = target_length # not used for now\n","        self.audio_processor = audio_processor\n","        prev_dia_id = None\n","        utt_count = 0\n","\n","        # Iterate through the dataframe to build the dialogues\n","        for _, row in df.iterrows():\n","            dia_id = row[\"Dialogue_ID\"]\n","            utt_id = row[\"Utterance_ID\"]\n","\n","            # Update max dialogue size\n","            if prev_dia_id == dia_id:\n","                utt_count += 1\n","            else:\n","                if utt_count > self.max_dialogue_size:\n","                    self.max_dialogue_size = utt_count\n","                utt_count = 1\n","\n","            # Get audio file path\n","            audio_file = f'{root_dir}/audio/dia{dia_id}_utt{utt_id}.wav'\n","\n","            # Load the waveform using torchaudio\n","            waveform, sr = torchaudio.load(audio_file)\n","            \n","            # If the audio is stereo (2 channels), convert it\n","            # to mono by averaging the channels\n","            if waveform.size(0) > 1:\n","                waveform = waveform.mean(dim=0, keepdim=True)\n","                \n","            # Apply audio transform if provided, otherwise use raw waveform\n","            if self.audio_transform:\n","                audio_features = self.audio_transform(waveform, sr)\n","            else:\n","                audio_features = waveform\n","                \n","            # Apply processor if provided\n","            if self.audio_processor:\n","                # Process waveform into log-Mel spectrogram using WhisperProcessor\n","                inputs = self.audio_processor.feature_extractor(\n","                    waveform.squeeze(0), sampling_rate=16000, return_tensors=\"pt\"\n","                )\n","                input_features = inputs.input_features[0]  # Extract the spectrogram\n","                audio_features = input_features\n","\n","            # Process emotion and sentiment labels\n","            emotion = row[\"Emotion\"].lower()\n","            emotion_int = self.emotion_to_int(emotion)\n","            sentiment = row[\"Sentiment\"].lower()\n","            sentiment_int = self.sentiment_to_int(sentiment)\n","\n","            # Update class counts\n","            self.emotion_class_counts[emotion_int] += 1\n","            self.sentiment_class_counts[sentiment_int] += 1\n","\n","            utter_dict = {\n","                \"audio_features\": audio_features,\n","                \"transcript\": row[\"Utterance\"],\n","                \"emotion\": emotion_int,\n","                \"sentiment\": sentiment_int\n","            }\n","\n","            # Add utterance to the corresponding dialogue\n","            if dia_id not in self.dialogues:\n","                self.dialogues[dia_id] = []\n","            self.dialogues[dia_id].append(utter_dict)\n","\n","            prev_dia_id = dia_id\n","\n","        # Convert to a list of (dialog_id, list_of_utterances)\n","        self.dialogues = [(k, v) for k, v in self.dialogues.items()]\n","\n","    def _pad_or_truncate(self, audio_features):\n","        \"\"\"\n","        Pad or truncate audio features to the target length.\n","\n","        :param audio_features: Tensor of audio features with shape [n_mels, time_steps].\n","        :return: Padded or truncated tensor with shape [n_mels, target_length].\n","        \"\"\"\n","        n_mels, n_frames = audio_features.shape\n","\n","        if n_frames < self.target_length:\n","            # Pad with zeros if the spectrogram is too short\n","            pad_amount = self.target_length - n_frames\n","            audio_features = torch.nn.functional.pad(\n","                audio_features, (0, pad_amount), mode=\"constant\", value=0)\n","        elif n_frames > self.target_length:\n","            # Truncate if the spectrogram is too long\n","            audio_features = audio_features[:, :self.target_length]\n","\n","        return audio_features\n","\n","    def emotion_to_int(self, emotion):\n","        \"\"\"\n","        Map emotion labels to integers.\n","        \"\"\"\n","        str_to_int = {\"neutral\": 0, \"joy\": 1, \"surprise\": 2,\n","                      \"anger\": 3, \"sadness\": 4, \"fear\": 5, \"disgust\": 6}\n","        return str_to_int[emotion]\n","\n","    def emotion_to_str(self, emotion_int):\n","        \"\"\"\n","        Map integers to emotion labels.\n","        \"\"\"\n","        int_to_str = {0: \"neutral\", 1: \"joy\", 2: \"surprise\",\n","                      3: \"anger\", 4: \"sadness\", 5: \"fear\", 6: \"disgust\"}\n","        return int_to_str[emotion_int]\n","\n","    def sentiment_to_int(self, sentiment):\n","        \"\"\"\n","        Map sentiment labels to integers.\n","        \"\"\"\n","        str_to_int = {\"neutral\": 0, \"positive\": 1, \"negative\": 2}\n","        return str_to_int[sentiment]\n","\n","    def sentiment_to_str(self, sentiment_int):\n","        \"\"\"\n","        Map integers to sentiment labels.\n","        \"\"\"\n","        int_to_str = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n","        return int_to_str[sentiment_int]\n","\n","    def __len__(self):\n","        \"\"\"\n","        Return the number of dialogues.\n","        \"\"\"\n","        return len(self.dialogues)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Get a dialogue by index.\n","        \"\"\"\n","        dialog_id, utterances = self.dialogues[idx]\n","\n","        # Prepare lists for audio features, transcripts, emotions, and sentiments\n","        audio_list = []\n","        text_list = []\n","        emotion_list = []\n","        sentiment_list = []\n","\n","        for utt in utterances:\n","            audio_list.append(utt[\"audio_features\"])\n","            text_list.append(utt[\"transcript\"])\n","            emotion_list.append(utt[\"emotion\"])\n","            sentiment_list.append(utt[\"sentiment\"])\n","\n","        return {\n","            \"dialog_id\": dialog_id,\n","            \"audio_list\": audio_list,\n","            \"text_list\": text_list,\n","            \"emotion_list\": emotion_list,\n","            \"sentiment_list\": sentiment_list\n","        }"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-01-07T23:08:42.844157Z","iopub.status.busy":"2025-01-07T23:08:42.842376Z","iopub.status.idle":"2025-01-07T23:08:42.857565Z","shell.execute_reply":"2025-01-07T23:08:42.856552Z","shell.execute_reply.started":"2025-01-07T23:08:42.844115Z"},"trusted":true},"outputs":[],"source":["# class MELDConversationDataset(Dataset):\n","#     def __init__(self, csv_file, root_dir='./data', text_processor=None, audio_processor=None):\n","#         \"\"\"\n","#         We'll store a list of (dialog_id, [list_of_utterance_dicts]).\n","#         Each utterance_dict might contain:\n","#           {\n","#             \"fbank_path\": str,\n","#             \"transcript\": str,\n","#             \"emotion\": int,\n","#             \"sentiment\": int\n","#           }\n","#         \"\"\"\n","#         df = pd.read_csv(f'{root_dir}/{csv_file}')\n","\n","#         # order the df rows according to dialogueID and each dialogue according to utteranceID\n","#         df = df.sort_values(by=['Dialogue_ID', 'Utterance_ID'])\n","\n","#         self.emotion_class_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n","#         self.sentiment_class_counts = {0: 0, 1: 0, 2: 0}\n","#         self.max_dialogue_size = 0\n","\n","#         self.dialogues = {}  # key: dialogue_id, value: list of utterance dicts\n","#         prev_dia_id = None\n","#         utt_count = 0\n","\n","#         for _, row in df.iterrows():\n","\n","#             dia_id = row[\"Dialogue_ID\"]\n","#             utt_id = row[\"Utterance_ID\"]\n","\n","#             if prev_dia_id == dia_id:\n","#                 utt_count += 1\n","#             else:\n","#                 if utt_count > self.max_dialogue_size:\n","#                     self.max_dialogue_size = utt_count\n","#                 utt_count = 1\n","\n","#             fbank_path = f'{root_dir}/fbank/dia{dia_id}_utt{utt_id}.npy'\n","            \n","#             # get the waveform file and convert it into\n","#             # mel spectrogram\n","#             audio_file = f'{root_dir}/audio/dia{dia_id}_utt{utt_id}.wav'\n","#             mel_spectrogram = waveform_to_logmel(waveform=torchaudio.load(audio_file)[0])\n","\n","#             emotion = row[\"Emotion\"]\n","#             emotion = emotion.lower()\n","#             emotion_int = self.emotion_to_int(emotion)\n","#             sentiment = row[\"Sentiment\"]\n","#             sentiment = sentiment.lower()\n","#             sentiment_int = self.sentiment_to_int(sentiment)\n","\n","#             self.emotion_class_counts[emotion_int] += 1\n","#             self.sentiment_class_counts[sentiment_int] += 1\n","\n","#             utter_dict = {\n","#                 \"fbank_path\": fbank_path,\n","#                 \"transcript\": row[\"Utterance\"],\n","#                 \"emotion\": emotion_int,\n","#                 \"sentiment\": sentiment_int\n","#             }\n","\n","#             if dia_id not in self.dialogues:\n","#                 self.dialogues[dia_id] = []\n","#             self.dialogues[dia_id].append(utter_dict)\n","\n","#             prev_dia_id = dia_id\n","\n","#         # Convert to list of (dialog_id, list_of_utterances)\n","#         self.dialogues = [(k, sorted(v, key=lambda x: x[\"fbank_path\"]))\n","#                           for k, v in self.dialogues.items()]\n","#         # The sorting step ensures the utterances are in ascending order of utt_id if needed.\n","\n","#     def emotion_to_int(self, str):\n","#         str_to_int = {\"neutral\": 0, \"joy\": 1, \"surprise\": 2,\n","#                       \"anger\": 3, \"sadness\": 4, \"fear\": 5, \"disgust\": 6}\n","#         return str_to_int[str]\n","\n","#     def emotion_to_str(self, int):\n","#         int_to_str = {0: \"neutral\", 1: \"joy\", 2: \"surprise\",\n","#                       3: \"anger\", 4: \"sadness\", 5: \"fear\", 6: \"disgust\"}\n","#         return int_to_str[int]\n","\n","#     def sentiment_to_int(self, str):\n","#         str_to_int = {\"neutral\": 0, \"positive\": 1, \"negative\": 2}\n","#         return str_to_int[str]\n","\n","#     def sentiment_to_str(self, int):\n","#         int_to_str = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n","#         return int_to_str[int]\n","\n","#     def __len__(self):\n","#         return len(self.dialogues)\n","\n","#     def __getitem__(self, idx):\n","#         dialog_id, utterances = self.dialogues[idx]\n","\n","#         # For each utterance, load filterbanks, transcript, emotion, sentiment\n","#         audio_list = []\n","#         text_list = []\n","#         emotion_list = []\n","#         sentiment_list = []\n","\n","#         for utt in utterances:\n","#             # shape e.g. (T, fbank_dim)\n","#             fbank = np.load(utt[\"fbank_path\"])\n","#             audio_list.append(fbank)\n","\n","#             text_list.append(utt[\"transcript\"])\n","#             emotion_list.append(utt[\"emotion\"])         # or mapped to int\n","#             sentiment_list.append(utt[\"sentiment\"])     # or mapped to int\n","\n","#         return {\n","#             \"dialog_id\": dialog_id,\n","#             \"audio_list\": audio_list,\n","#             \"text_list\": text_list,\n","#             \"emotion_list\": emotion_list,\n","#             \"sentiment_list\": sentiment_list\n","#         }"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class AudioTransformPipeline:\n","    \"\"\"\n","    Custom pipeline for chaining multiple audio transformations with dynamic resampling.\n","    \"\"\"\n","\n","    def __init__(self, target_sample_rate=16000, n_mels=128):\n","        self.target_sample_rate = target_sample_rate\n","        self.n_mels = n_mels\n","\n","    def __call__(self, waveform, orig_sample_rate):\n","        audio = waveform\n","        \n","        # Step 1: Resample if the original sample rate is different\n","        if orig_sample_rate != self.target_sample_rate:\n","            resampler = T.Resample(\n","                orig_freq=orig_sample_rate, new_freq=self.target_sample_rate)\n","            audio = resampler(audio)\n","\n","        # Step 2: Convert to Mel Spectrogram\n","        # mel_transform = T.MelSpectrogram(\n","        #     sample_rate=self.target_sample_rate, n_mels=self.n_mels)\n","        # mel_spec = mel_transform(waveform)\n","\n","        # Step 3: Convert amplitude to decibels\n","        # db_transform = T.AmplitudeToDB()\n","        # mel_spec_db = db_transform(mel_spec)\n","        \n","        return audio\n","\n","def log_mel_transform(waveform, sample_rate=16000, n_fft=400, hop_length=160, n_mels=128):\n","    \"\"\"\n","    Convert a waveform to a log Mel spectrogram using torchaudio.transforms.\n","\n","    :param waveform: Tensor of shape [channels, time] or [time].\n","    :param sample_rate: Sampling rate of the input waveform (default: 16000).\n","    :param n_fft: Number of FFT bins (default: 400).\n","    :param hop_length: Hop length between frames (default: 160).\n","    :param n_mels: Number of Mel filterbank channels (default: 128).\n","    :return: Log Mel spectrogram tensor of shape [n_mels, time_steps].\n","    \"\"\"\n","    # Ensure the waveform is 2D (channels, time)\n","    if len(waveform.shape) == 1:  # If 1D, add a channel dimension\n","        waveform = waveform.unsqueeze(0)\n","\n","    # Downmix to mono if more than 1 channel\n","    if waveform.shape[0] > 1:\n","        waveform = torch.mean(waveform, dim=0, keepdim=True)\n","\n","    # Define the MelSpectrogram transform\n","    mel_spectrogram_transform = T.MelSpectrogram(\n","        sample_rate=sample_rate,\n","        n_fft=n_fft,\n","        hop_length=hop_length,\n","        n_mels=n_mels,\n","        power=2.0  # Use power spectrogram (not amplitude)\n","    )\n","\n","    # Apply the MelSpectrogram transform\n","    mel_spectrogram = mel_spectrogram_transform(\n","        waveform)  # Shape: [1, n_mels, time_steps]\n","\n","    # Take the log of the Mel spectrogram, ensuring no log(0) or negative values\n","    log_mel = torch.log(mel_spectrogram + 1e-6)  # Add epsilon to avoid log(0)\n","\n","    return log_mel.squeeze(0)  # Remove the channel dimension for consistency\n","\n","def log_mel_transform_fn(waveform): return log_mel_transform(\n","    waveform, sample_rate=16000)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-01-07T23:08:42.859145Z","iopub.status.busy":"2025-01-07T23:08:42.858750Z","iopub.status.idle":"2025-01-07T23:08:58.746310Z","shell.execute_reply":"2025-01-07T23:08:58.745266Z","shell.execute_reply.started":"2025-01-07T23:08:42.859108Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Data loaded.\n","Data loaders created.\n"]}],"source":["# Define the custom transformation pipeline\n","audio_processor = AutoProcessor.from_pretrained(audio_model_name)\n","audio_transform = AudioTransformPipeline()\n","\n","train_set = MELDConversationDataset(\n","    csv_file=\"train_sent_emo.csv\", \n","    root_dir=\"data/train\",\n","    audio_transform=audio_transform,\n","    audio_processor=audio_processor\n",")\n","dev_set = MELDConversationDataset(\n","    csv_file=\"dev_sent_emo.csv\", \n","    root_dir=\"data/dev\",\n","    audio_transform=audio_transform,\n","    audio_processor=audio_processor\n",")\n","test_set = MELDConversationDataset(\n","    csv_file='test_sent_emo.csv', \n","    root_dir='data/test',\n","    audio_transform=audio_transform,\n","    audio_processor=audio_processor\n",")\n","\n","print(\"Data loaded.\")\n","\n","# train_loader = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=4, collate_fn=meld_collate_fn)\n","# dev_loader = DataLoader(dev_set, batch_size=4, shuffle=True, num_workers=4, collate_fn=meld_collate_fn)\n","# test_loader = DataLoader(test_set, batch_size=4, shuffle=True, num_workers=4, collate_fn=meld_collate_fn)\n","\n","print(\"Data loaders created.\")"]},{"cell_type":"markdown","metadata":{},"source":["### Miscs"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1038 114 169\n"]}],"source":["print(len(train_set), len(dev_set), len(test_set))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T16:51:34.622185Z","iopub.status.busy":"2025-01-06T16:51:34.621837Z","iopub.status.idle":"2025-01-06T16:51:34.628399Z","shell.execute_reply":"2025-01-06T16:51:34.627441Z","shell.execute_reply.started":"2025-01-06T16:51:34.622152Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of emotions: 7\n","Number of sentiments: 3\n","Emotion neutral count: 4709\n","Emotion joy count: 1743\n","Emotion surprise count: 1205\n","Emotion anger count: 1109\n","Emotion sadness count: 683\n","Emotion fear count: 268\n","Emotion disgust count: 271\n","Sentiment neutral count: 4709\n","Sentiment positive count: 2334\n","Sentiment negative count: 2945\n","\n","\n"]}],"source":["num_emotions = len(train_set.emotion_class_counts)\n","num_sentiments = len(train_set.sentiment_class_counts)\n","\n","print(\"Number of emotions:\", num_emotions)\n","print(\"Number of sentiments:\", num_sentiments)\n","\n","for i in range(7):\n","    print(f\"Emotion {train_set.emotion_to_str(i)} count:\", train_set.emotion_class_counts[i])\n","for i in range(3):\n","    print(f\"Sentiment {train_set.sentiment_to_str(i)} count:\", train_set.sentiment_class_counts[i])\n","    \n","print(\"\\n\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total samples: 9988\n","Class weights: tensor([0.0186, 0.0503, 0.0728, 0.0791, 0.1284, 0.3272, 0.3236])\n"]}],"source":["#get weights for balancing classes\n","class_counts = train_set.emotion_class_counts\n","total_samples = 0\n","\n","for key in class_counts:\n","    total_samples += class_counts[key]\n","    \n","print(\"Total samples:\", total_samples)\n","class_weights = torch.zeros(len(class_counts))\n","\n","for i in range(len(class_counts)):\n","    class_weights[i] = class_counts[i] / total_samples\n","    \n","class_weights = 1 / class_weights  # invert the weights\n","class_weights = class_weights / class_weights.sum()  # normalize the weights\n","class_weights = class_weights.to(device)\n","print(\"Class weights:\", class_weights)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Max dialogue size: 24\n"]}],"source":["# set max utterance size to be the max of all train dev and test sets\n","max_utt = max(train_set.max_dialogue_size,\n","              dev_set.max_dialogue_size, test_set.max_dialogue_size)\n","print(\"Max dialogue size:\", max_utt)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# display mel spectrogram from an example\n","def visualize_mel_spectrogram(mel_spectrogram):\n","    plt.figure(figsize=(10, 4))\n","    plt.imshow(mel_spectrogram, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n","    plt.colorbar()\n","    plt.title(\"Mel Spectrogram\")\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Embeddings"]},{"cell_type":"markdown","metadata":{},"source":["### Encoders"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["from transformers import AutoProcessor, AutoModel, WhisperProcessor, WhisperModel\n","import torch\n","import torch.nn as nn\n","\n","\n","class AudioEncoder(nn.Module):\n","    def __init__(self, model_name, preprocessing=True, fine_tune=False, unfreeze_last_n_layers=2):\n","        \"\"\"\n","        Generalized audio encoder for both encoder-only and encoder-decoder models.\n","\n","        :param model_name: Hugging Face model name (e.g., \"facebook/wav2vec2-base\", \"openai/whisper-small\").\n","        :param preprocessing: Whether to apply preprocessing using a processor.\n","        :param fine_tune: Whether to fine-tune the model.\n","        :param unfreeze_last_n: Number of encoder layers to unfreeze if fine-tuning.\n","        \"\"\"\n","        super(AudioEncoder, self).__init__()\n","\n","        # Preprocessing processor (if required)\n","        self.processor = AutoProcessor.from_pretrained(\n","            model_name) if preprocessing else None\n","\n","        # Load the model dynamically\n","        self.model = AutoModel.from_pretrained(model_name)\n","\n","        # Detect if the model is encoder-only or encoder-decoder\n","        self.is_encoder_decoder = hasattr(\n","            self.model, \"encoder\") and hasattr(self.model, \"decoder\")\n","\n","        # Freeze the entire model by default\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","\n","        # Fine-tune the last `unfreeze_last_n` encoder layers, if specified\n","        if fine_tune:\n","            if self.is_encoder_decoder:\n","                # For encoder-decoder models (e.g., Whisper)\n","                total_layers = len(self.model.encoder.layers)\n","                for layer_idx in range(total_layers - unfreeze_last_n_layers, total_layers):\n","                    for param in self.model.encoder.layers[layer_idx].parameters():\n","                        param.requires_grad = True\n","            elif hasattr(self.model, \"encoder\"):\n","                # For encoder-only models (e.g., Wav2Vec2, HuBERT)\n","                total_layers = len(self.model.encoder.layers)\n","                for layer_idx in range(total_layers - unfreeze_last_n_layers, total_layers):\n","                    for param in self.model.encoder.layers[layer_idx].parameters():\n","                        param.requires_grad = True\n","            else:\n","                print(\n","                    f\"Warning: Fine-tuning is not supported for {model_name}\")\n","\n","    def preprocess_audio(self, audios, sampling_rate=16000):\n","        \"\"\"\n","        Preprocess audio using the processor, if available.\n","\n","        :param audios: Tensor of raw audio waveforms (shape: [batch_size, num_samples]).\n","        :param sampling_rate: Sampling rate of the audio (default: 16 kHz).\n","        :return: Preprocessed inputs for the model.\n","        \"\"\"\n","        if self.processor:\n","            if hasattr(self.processor, \"feature_extractor\"):\n","                print(audios.shape)\n","                # For models like Whisper that require log-Mel spectrograms\n","                inputs = self.processor.feature_extractor(\n","                    audios.cpu().numpy(),  # Convert PyTorch tensor to NumPy array\n","                    sampling_rate=sampling_rate,\n","                    return_tensors=\"pt\",\n","                    padding=True\n","                )\n","            else:\n","                # For models like Wav2Vec2 that directly process raw waveforms\n","                inputs = self.processor(\n","                    audios.cpu(),  # Raw tensor (no conversion needed)\n","                    sampling_rate=sampling_rate,\n","                    return_tensors=\"pt\",\n","                    padding=True\n","                )\n","        else:\n","            # If no preprocessing is required, assume the input is already processed\n","            return {\"input_features\": audios}\n","\n","        return inputs\n","\n","    def forward(self, audios):\n","        \"\"\"\n","        Forward pass to process audio and extract embeddings.\n","\n","        :param audios: Tensor of raw audio waveforms (shape: [batch_size, num_samples]).\n","        :return: Mean-pooled audio embeddings (shape: [batch_size, hidden_size]).\n","        \"\"\"\n","        # Preprocess the audio if required\n","        inputs = self.preprocess_audio(audios)\n","\n","        # Move inputs to the same device as the model\n","        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n","\n","        # Forward pass\n","        with torch.no_grad() if not self.training or not any(\n","            p.requires_grad for p in self.model.parameters()\n","        ) else torch.enable_grad():\n","            if self.is_encoder_decoder:\n","                # Encoder-decoder models (e.g., Whisper)\n","                encoder_outputs = self.model.encoder(inputs[\"input_features\"])\n","                hidden_states = encoder_outputs.last_hidden_state\n","            elif hasattr(self.model, \"encoder\"):\n","                # Encoder-only models (e.g., Wav2Vec2, HuBERT)\n","                outputs = self.model(**inputs)\n","                hidden_states = outputs.last_hidden_state\n","            else:\n","                raise ValueError(\n","                    f\"Model {self.model.config.model_type} is not supported.\")\n","\n","        # Mean pooling over the time dimension to get fixed-size embeddings\n","        # shape: [batch_size, hidden_size]\n","        audio_emb = hidden_states.mean(dim=1)\n","\n","        return audio_emb\n","\n","class TextEncoder(nn.Module):\n","    \"\"\"\n","    Generalized text encoder for transformer-based models (e.g., BERT, DistilBERT, RoBERTa).\n","    \"\"\"\n","\n","    def __init__(self,\n","                 model_name=\"distilbert-base-uncased\",\n","                 fine_tune=False,\n","                 unfreeze_last_n_layers=2):\n","        super(TextEncoder, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModel.from_pretrained(model_name)\n","\n","        # Freeze the entire model by default\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","\n","        if fine_tune and unfreeze_last_n_layers > 0:\n","            # Dynamically detect the encoder layers based on model architecture\n","            if hasattr(self.model, \"encoder\") and hasattr(self.model.encoder, \"layer\"):\n","                # For models like BERT, RoBERTa, etc.\n","                total_layers = len(self.model.encoder.layer)\n","                for layer_idx in range(total_layers - unfreeze_last_n_layers, total_layers):\n","                    for param in self.model.encoder.layer[layer_idx].parameters():\n","                        param.requires_grad = True\n","            elif hasattr(self.model, \"transformer\") and hasattr(self.model.transformer, \"layer\"):\n","                # For models like DistilBERT\n","                total_layers = len(self.model.transformer.layer)\n","                for layer_idx in range(total_layers - unfreeze_last_n_layers, total_layers):\n","                    for param in self.model.transformer.layer[layer_idx].parameters():\n","                        param.requires_grad = True\n","            else:\n","                raise ValueError(\n","                    f\"Unfreezing layers is not supported for the model architecture of {\n","                        model_name}\"\n","                )\n","\n","            # Optionally unfreeze the pooler layer if it exists\n","            if hasattr(self.model, \"pooler\"):\n","                for param in self.model.pooler.parameters():\n","                    param.requires_grad = True\n","\n","    def forward(self, text_list):\n","        \"\"\"\n","        :param text_list: A list of strings (or a single string) to encode.\n","        :return: A tensor of shape [batch_size, hidden_dim] with text embeddings.\n","        \"\"\"\n","        device = self.model.device\n","\n","        # If a single string is passed, wrap it into a list\n","        if isinstance(text_list, str):\n","            text_list = [text_list]\n","\n","        # Tokenize the input text\n","        encodings = self.tokenizer(\n","            text_list,\n","            padding=True,\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        ).to(device)\n","\n","        # If any parameters require gradients, use torch.enable_grad()\n","        use_grad = any(p.requires_grad for p in self.model.parameters())\n","        with torch.enable_grad() if use_grad else torch.no_grad():\n","            outputs = self.model(\n","                input_ids=encodings.input_ids,\n","                attention_mask=encodings.attention_mask\n","            )\n","\n","        # Extract the [CLS] token embedding from the last hidden state\n","        # outputs.last_hidden_state -> shape [batch_size, seq_len, hidden_dim]\n","        # shape [batch_size, hidden_dim]\n","        cls_emb = outputs.last_hidden_state[:, 0, :]\n","        return cls_emb"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset class for embeddings"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["class DialogueEmbeddingDataset(Dataset):\n","    def __init__(self, audio_embs, text_embs, emotion_labels, sentiment_labels, dialogue_ids):\n","        \"\"\"\n","        :param audio_embs:       List of FloatTensors [num_dialogues, num_utterances, audio_dim]\n","        :param text_embs:        List of FloatTensors [num_dialogues, num_utterances, text_dim]\n","        :param emotion_labels:   List of lists (dialogue-level emotion labels) [num_dialogues, num_utterances]\n","        :param sentiment_labels: List of lists (dialogue-level sentiment labels) [num_dialogues, num_utterances]\n","        :param dialogue_ids:     List of dialogue IDs [num_dialogues]\n","        \"\"\"\n","        self.audio_embs = audio_embs\n","        self.text_embs = text_embs\n","        self.emotion_labels = emotion_labels\n","        self.sentiment_labels = sentiment_labels\n","        self.dialogue_ids = dialogue_ids\n","\n","    def __len__(self):\n","        return len(self.audio_embs)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"dialog_id\": self.dialogue_ids[idx],\n","            \"audio_lists\": self.audio_embs[idx],\n","            \"text_lists\": self.text_embs[idx],\n","            \"emotion_lists\": self.emotion_labels[idx],\n","            \"sentiment_lists\": self.sentiment_labels[idx]\n","        }\n","\n","\n","def build_embedding_dataset(dataset: Dataset, audio_encoder: AudioEncoder, text_encoder: TextEncoder) -> Dataset:\n","    \"\"\"\n","    Processes a MELDConversationDataset, computes audio and text embeddings for each utterance\n","    in all dialogues, and returns a new dataset with embeddings instead of raw features.\n","    \n","    :param dataset: Instance of MELDConversationDataset (raw dataset with FBanks and transcripts).\n","    :param audio_encoder: Pretrained Whisper model wrapped in an AudioEncoder class.\n","    :param text_encoder: Pretrained text encoder (e.g., BERT or similar).\n","    :param device: Device to perform inference on (default: 'cuda').\n","    :return: New dataset with audio and text embeddings.\n","    \"\"\"\n","    # Ensure the encoders are in evaluation mode\n","    audio_encoder.eval()\n","    text_encoder.eval()\n","\n","    all_dialogue_audio_embs = []\n","    all_dialogue_text_embs = []\n","    all_emotion_labels = []\n","    all_sentiment_labels = []\n","    all_dialogue_ids = []\n","\n","    for idx in range(len(dataset)):\n","        data = dataset[idx]\n","        dialog_id = data[\"dialog_id\"]\n","        audio_list = data[\"audio_list\"]\n","        text_list = data[\"text_list\"]\n","        emotion_list = data[\"emotion_list\"]\n","        sentiment_list = data[\"sentiment_list\"]\n","\n","        dialogue_audio_embs = []\n","        dialogue_text_embs = []\n","\n","        with torch.no_grad():\n","            for audio, text in zip(audio_list, text_list):\n","\n","                # Step 2: Compute audio embedding using Whisper model\n","                audio_emb = audio_encoder(audio.unsqueeze(0).to(device))\n","                dialogue_audio_embs.append(audio_emb.cpu())\n","\n","                # Step 3: Compute text embedding using the text encoder\n","                # Replace with your text encoder (e.g., BERT)\n","                text_emb = text_encoder(text)\n","                # Remove batch dimension if present\n","                text_emb = text_emb.squeeze(0)\n","                dialogue_text_embs.append(text_emb.cpu())\n","\n","        # Stack embeddings for all utterances in the dialogue\n","        s = torch.stack(dialogue_audio_embs)\n","        dialogue_text_embs = torch.stack(dialogue_text_embs)\n","\n","        # Append dialogue-level data\n","        all_dialogue_audio_embs.append(dialogue_audio_embs)\n","        all_dialogue_text_embs.append(dialogue_text_embs)\n","        all_emotion_labels.append(emotion_list)\n","        all_sentiment_labels.append(sentiment_list)\n","        all_dialogue_ids.append(dialog_id)\n","\n","    # Build the new DialogueEmbeddingDataset\n","    emb_dataset = DialogueEmbeddingDataset(\n","        audio_embs=all_dialogue_audio_embs,\n","        text_embs=all_dialogue_text_embs,\n","        emotion_labels=all_emotion_labels,\n","        sentiment_labels=all_sentiment_labels,\n","        dialogue_ids=all_dialogue_ids\n","    )\n","    return emb_dataset"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["\n","# audio_encoder = AudioEncoder(model_name=audio_model_name)\n","# text_encoder = TextEncoder(model_name=text_model_name)\n","# audio_encoder = AudioEncoder(model_name=audio_model_name, preprocessing=False)"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["# train_emb_dataset = build_embedding_dataset(train_set, audio_encoder, text_encoder)\n","# dev_emb_dataset = build_embedding_dataset(dev_set, audio_encoder, text_encoder)\n","# test_emb_dataset = build_embedding_dataset(test_set, audio_encoder, text_encoder)"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["# PARAMETERS\n","lr = 1e-4\n","criterions = {\n","    'emotion': nn.CrossEntropyLoss(weight=class_weights),\n","    'sentiment': nn.CrossEntropyLoss()\n","}\n","num_epochs = 50"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None, dropout_p=0.5):\n","        super(BasicBlock, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError(\n","                'BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\n","                \"Dilation > 1 not supported in BasicBlock\")\n","\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.dropout = nn.Dropout(p=dropout_p)  # Dropout layer added\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.dropout(out)  # apply dropout after first ReLU\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","        out = self.dropout(out)  # apply dropout after second ReLU\n","\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["### BasicBlock class"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["class ResNet(nn.Module):\n","\n","    def __init__(self, block, layers, modality, num_classes=1000, pool='avgpool', zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n","                 norm_layer=None):\n","        super(ResNet, self).__init__()\n","        self.modality = modality\n","        self.pool = pool\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        if modality == 'audio':\n","            self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n","                                   bias=False)\n","        elif modality == 'visual':\n","            self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n","                                   bias=False)\n","        else:\n","            raise NotImplementedError(\n","                'Incorrect modality, should be audio or visual but got {}'.format(modality))\n","\n","        self.out_conv = nn.Conv2d(\n","            512 * block.expansion, 768, kernel_size=1, stride=1, bias=False)\n","\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(\n","                    m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.normal_(m.weight, mean=1, std=0.02)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","\n","        if self.modality == 'visual':\n","            (B, C, T, H, W) = x.size()\n","            x = x.permute(0, 2, 1, 3, 4).contiguous()\n","            x = x.view(B * T, C, H, W)\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.out_conv(x)  # Now x has shape [B, 768, H_out, W_out]\n","\n","        out = x\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(Bottleneck, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","def _resnet(arch, block, layers, modality, progress, **kwargs):\n","    model = ResNet(block, layers, modality, **kwargs)\n","    return model\n","\n","\n","def resnet18(modality, progress=True, **kwargs):\n","    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], modality, progress,\n","                   **kwargs)"]},{"cell_type":"markdown","metadata":{},"source":["### Actual model"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["class FusionLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=256, num_layers=1, bidirectional=False, dropout=0.1):\n","        super(FusionLSTM, self).__init__()\n","        self.lstm = nn.LSTM(\n","            input_size=input_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            bidirectional=bidirectional,\n","            dropout=dropout if num_layers > 1 else 0.0\n","        )\n","        self.hidden_dim = hidden_dim * (2 if bidirectional else 1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (batch_size, seq_len, input_dim)\n","        returns: (batch_size, seq_len, hidden_dim)\n","        \"\"\"\n","        output, (hn, cn) = self.lstm(x)\n","        return output, (hn, cn)  # output is the sequence of hidden states\n","\n","\n","class MultimodalClassifierWithLSTM(nn.Module):\n","    def __init__(self, \n","        audio_encoder, text_encoder, fusion_lstm, \n","        hidden_dim=256, num_emotions=7, num_sentiments=3, \n","        max_utt=50, compute_emb=False):\n","        super(MultimodalClassifierWithLSTM, self).__init__()\n","        self.audio_encoder = audio_encoder\n","        self.text_encoder = text_encoder\n","        self.fusion_lstm = fusion_lstm  # e.g., an instance of FusionLSTM\n","        self.emotion_head = nn.Linear(hidden_dim*2, num_emotions)\n","        self.sentiment_head = nn.Linear(hidden_dim*2, num_sentiments)\n","        self.max_utt = max_utt\n","        self.compute_emb = compute_emb\n","\n","    def forward(self, audio_dialog_utts, text_dialog_utts):\n","\n","        if self.compute_emb:\n","            audio_emb = self.audio_encoder(audio_dialog_utts.unsqueeze(0))\n","            # audio_emb = F.adaptive_avg_pool2d(audio_emb, 1)\n","            audio_emb = torch.flatten(audio_emb, 1)\n","\n","            text_emb = [self.text_encoder(utt) for utt in text_dialog_utts]\n","            text_emb = torch.stack(text_emb)  # shape (B, hidden_dim)\n","            \n","            print(\"Audio emb shape: \", audio_emb.shape)\n","            print(\"Text emb shape: \", text_emb.shape)\n","\n","            text_emb = text_emb.squeeze(1)\n","        else:\n","            audio_emb = audio_dialog_utts\n","            text_emb = text_dialog_utts\n","\n","        # print(\"Final Audio emb shape: \", audio_emb.shape)\n","        # print(\"Final Text emb shape: \", text_emb.shape)\n","\n","        # Combine\n","        # (utts, audio_enc_dim + text_enc_dim)\n","        \n","        fused_emb = torch.cat([audio_emb, text_emb], dim=-1)\n","\n","        # Reshape back to (B, S, fused_dim) for the LSTM\n","        # (1, utts, audio_enc_dim + text_enc_dim)\n","        fused_emb = fused_emb.unsqueeze(0)\n","\n","        padded_fused_emb = F.pad(\n","            fused_emb, (0, 0, 0, self.max_utt - fused_emb.size(1)))\n","\n","        # Pass through LSTM\n","        lstm_out, (hn, cn) = self.fusion_lstm(padded_fused_emb)\n","\n","        # print(\"LSTM out shape: \", lstm_out.shape)\n","\n","        lstm_out = lstm_out.squeeze(0)\n","\n","        # Classification heads\n","\n","        emotion_logits = self.emotion_head(lstm_out)\n","        sentiment_logits = self.sentiment_head(lstm_out)\n","\n","        return emotion_logits, sentiment_logits"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["text_encoder = TextEncoder(model_name=text_model_name, fine_tune=True, unfreeze_last_n_layers=1)\n","audio_encoder = AudioEncoder(model_name=audio_model_name, fine_tune=True, unfreeze_last_n_layers=1)\n","model = MultimodalClassifierWithLSTM(\n","    fusion_lstm=FusionLSTM(input_dim=1536, hidden_dim=256, num_layers=1, bidirectional=True, dropout=0.2),\n","    audio_encoder=audio_encoder,\n","    text_encoder=text_encoder,\n","    hidden_dim=256,\n","    num_emotions=num_emotions,\n","    num_sentiments=num_sentiments,\n","    max_utt=max_utt,\n","    compute_emb=True\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)\n","experiment_name = 'TEST'"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["def train_one_epoch(model, dataloader, optimizer, criterions, emotion_reg=0.5, sentiment_reg=0.5, device='cuda'):\n","    model.train()\n","\n","    losses = {'emotion': 0.0, 'sentiment': 0.0}\n","    metrics = {\n","        'emotion': {'fused': [], 'labels': []},\n","        'sentiment': {'fused': [], 'labels': []}\n","    }\n","\n","    loop = tqdm(dataloader, desc=\"Training\", leave=False)\n","\n","    for batch in loop:\n","        batch_emotion_loss = 0.0\n","        batch_sentiment_loss = 0.0\n","\n","        batch_emotion_preds = []\n","        batch_emotion_labels = []\n","        batch_sentiment_preds = []\n","        batch_sentiment_labels = []\n","\n","        for b_idx in range(len(batch[\"dialog_ids\"])):\n","            audio_list = batch[\"audio_lists\"][b_idx]\n","            text_list = batch[\"text_lists\"][b_idx]\n","            emotion_list = batch[\"emotion_lists\"][b_idx]\n","            sentiment_list = batch[\"sentiment_lists\"][b_idx]\n","\n","            # Determine the actual number of utterances before padding\n","            # Number of real utterances in this conversation\n","            actual_len = len(emotion_list)\n","\n","            # Convert lists to tensors\n","            emotion_tensor = torch.as_tensor(\n","                emotion_list, dtype=torch.long, device=device)\n","            sentiment_tensor = torch.as_tensor(\n","                sentiment_list, dtype=torch.long, device=device)\n","            audio_array = torch.as_tensor(\n","                audio_list, dtype=torch.float, device=device)\n","\n","            # audio_array = audio_array.unsqueeze(1)  # adjust dimensions as needed\n","\n","            # Forward pass\n","            emotion_logits, sentiment_logits = model(audio_array, text_list)\n","\n","            # If outputs have shape (1, seq_len, num_classes), squeeze the batch dimension\n","            if len(emotion_logits.shape) == 3:\n","                emotion_logits = emotion_logits.squeeze(0)\n","                sentiment_logits = sentiment_logits.squeeze(0)\n","\n","            # Slice the logits and tensors to ignore padded timesteps\n","            # emotion_logits shape assumed to be (max_utt, num_classes) at this point\n","            emotion_logits = emotion_logits[:actual_len]\n","            sentiment_logits = sentiment_logits[:actual_len]\n","\n","            emotion_tensor = emotion_tensor[:actual_len]\n","            sentiment_tensor = sentiment_tensor[:actual_len]\n","\n","            # Compute Loss\n","            e_loss = criterions['emotion'](emotion_logits, emotion_tensor)\n","            s_loss = criterions['sentiment'](\n","                sentiment_logits, sentiment_tensor)\n","\n","            batch_emotion_loss += e_loss\n","            batch_sentiment_loss += s_loss\n","\n","            # Predictions for non-padded time steps\n","            e_preds = torch.argmax(\n","                emotion_logits, dim=-1).detach().cpu().numpy()\n","            s_preds = torch.argmax(\n","                sentiment_logits, dim=-1).detach().cpu().numpy()\n","\n","            batch_emotion_preds.extend(e_preds)\n","            batch_emotion_labels.extend(emotion_list)\n","\n","            batch_sentiment_preds.extend(s_preds)\n","            batch_sentiment_labels.extend(sentiment_list)\n","\n","        # Average and combine losses over conversations in batch\n","        batch_emotion_loss /= len(batch[\"dialog_ids\"])\n","        batch_sentiment_loss /= len(batch[\"dialog_ids\"])\n","\n","        combined_loss = emotion_reg * batch_emotion_loss + \\\n","            sentiment_reg * batch_sentiment_loss\n","        optimizer.zero_grad()\n","        combined_loss.backward()\n","        optimizer.step()\n","\n","        losses['emotion'] += batch_emotion_loss.item()\n","        losses['sentiment'] += batch_sentiment_loss.item()\n","\n","        metrics['emotion']['fused'].extend(batch_emotion_preds)\n","        metrics['emotion']['labels'].extend(batch_emotion_labels)\n","\n","        metrics['sentiment']['fused'].extend(batch_sentiment_preds)\n","        metrics['sentiment']['labels'].extend(batch_sentiment_labels)\n","\n","    losses['emotion'] /= len(dataloader)\n","    losses['sentiment'] /= len(dataloader)\n","\n","    emotion_metrics = compute_metrics(\n","        metrics['emotion']['labels'], metrics['emotion']['fused'])\n","    sentiment_metrics = compute_metrics(\n","        metrics['sentiment']['labels'], metrics['sentiment']['fused'])\n","\n","    return losses, {\n","        'emotion': {'fused': emotion_metrics},\n","        'sentiment': {'fused': sentiment_metrics}\n","    }\n","\n","def validate_one_epoch(model, dataloader, criterions, device='cuda'):\n","    model.eval()\n","\n","    losses = {'emotion': 0.0, 'sentiment': 0.0}\n","    metrics = {\n","        'emotion': {'fused': [], 'labels': []},\n","        'sentiment': {'fused': [], 'labels': []}\n","    }\n","\n","    with torch.no_grad():\n","        loop = tqdm(dataloader, desc=\"Validation\", leave=False)\n","\n","        for batch in loop:\n","            batch_emotion_loss = 0.0\n","            batch_sentiment_loss = 0.0\n","\n","            batch_emotion_preds = []\n","            batch_emotion_labels = []\n","            batch_sentiment_preds = []\n","            batch_sentiment_labels = []\n","\n","            for b_idx in range(len(batch[\"dialog_ids\"])):\n","                audio_list = batch[\"audio_lists\"][b_idx]\n","                text_list = batch[\"text_lists\"][b_idx]\n","                emotion_list = batch[\"emotion_lists\"][b_idx]\n","                sentiment_list = batch[\"sentiment_lists\"][b_idx]\n","\n","                # Determine the actual number of utterances before padding\n","                actual_len = len(emotion_list)\n","\n","                # Convert lists to tensors\n","                emotion_tensor = torch.as_tensor(\n","                    emotion_list, dtype=torch.long, device=device)\n","                sentiment_tensor = torch.as_tensor(\n","                    sentiment_list, dtype=torch.long, device=device)\n","                audio_array = torch.as_tensor(\n","                    audio_list, dtype=torch.float, device=device)\n","\n","                audio_array = audio_array.unsqueeze(\n","                    1)  # adjust dimensions as needed\n","\n","                # Forward pass\n","                emotion_logits, sentiment_logits = model(\n","                    audio_array, text_list)\n","\n","                # If outputs have shape (1, seq_len, num_classes), squeeze the batch dimension\n","                if len(emotion_logits.shape) == 3:\n","                    emotion_logits = emotion_logits.squeeze(0)\n","                    sentiment_logits = sentiment_logits.squeeze(0)\n","\n","                # Slice the logits and tensors to ignore padded timesteps\n","                emotion_logits = emotion_logits[:actual_len]\n","                sentiment_logits = sentiment_logits[:actual_len]\n","\n","                emotion_tensor = emotion_tensor[:actual_len]\n","                sentiment_tensor = sentiment_tensor[:actual_len]\n","\n","                # Compute Loss\n","                e_loss = criterions['emotion'](emotion_logits, emotion_tensor)\n","                s_loss = criterions['sentiment'](\n","                    sentiment_logits, sentiment_tensor)\n","\n","                batch_emotion_loss += e_loss\n","                batch_sentiment_loss += s_loss\n","\n","                # Predictions for non-padded time steps\n","                e_preds = torch.argmax(\n","                    emotion_logits, dim=-1).detach().cpu().numpy()\n","                s_preds = torch.argmax(\n","                    sentiment_logits, dim=-1).detach().cpu().numpy()\n","\n","                batch_emotion_preds.extend(e_preds)\n","                batch_emotion_labels.extend(emotion_list)\n","\n","                batch_sentiment_preds.extend(s_preds)\n","                batch_sentiment_labels.extend(sentiment_list)\n","\n","            # Average and combine losses over conversations in batch\n","            batch_emotion_loss /= len(batch[\"dialog_ids\"])\n","            batch_sentiment_loss /= len(batch[\"dialog_ids\"])\n","\n","            losses['emotion'] += batch_emotion_loss.item()\n","            losses['sentiment'] += batch_sentiment_loss.item()\n","\n","            metrics['emotion']['fused'].extend(batch_emotion_preds)\n","            metrics['emotion']['labels'].extend(batch_emotion_labels)\n","\n","            metrics['sentiment']['fused'].extend(batch_sentiment_preds)\n","            metrics['sentiment']['labels'].extend(batch_sentiment_labels)\n","\n","        losses['emotion'] /= len(dataloader)\n","        losses['sentiment'] /= len(dataloader)\n","\n","        emotion_metrics = compute_metrics(\n","            metrics['emotion']['labels'], metrics['emotion']['fused'])\n","        sentiment_metrics = compute_metrics(\n","            metrics['sentiment']['labels'], metrics['sentiment']['fused'])\n","\n","    return losses, {\n","        'emotion': {'fused': emotion_metrics},\n","        'sentiment': {'fused': sentiment_metrics}\n","    }\n","\n","def compute_metrics(true_labels, predictions):\n","    \"\"\"\n","    Compute classification metrics including accuracy, per-class F1 scores, and weighted average F1 score.\n","    \"\"\"\n","    # Compute overall accuracy\n","    overall_accuracy = balanced_accuracy_score(true_labels, predictions)\n","\n","    # Compute F1 scores\n","    report = classification_report(\n","        true_labels, predictions, output_dict=True, zero_division=0\n","    )\n","    per_class_f1 = {label: values[\"f1-score\"]\n","                    for label, values in report.items() if label.isdigit()}\n","    macro_f1 = report[\"macro avg\"][\"f1-score\"]\n","\n","    # Compile metrics into a dictionary\n","    metrics = {\n","        \"balanced_acc\": overall_accuracy,\n","        \"macro_f1\": macro_f1,\n","        \"per_class_f1\": per_class_f1,\n","    }\n","\n","    return metrics\n","\n","def train_and_validate(model, train_loader, val_loader, optimizer, criterions, num_epochs, experiment_name, device='cuda', save_dir='./results'):\n","    \"\"\"\n","    Train and validate the model for multiple epochs, and display results in a summary table at the end.\n","\n","    :param model: The model to train.\n","    :param train_dataloader: Training DataLoader.\n","    :param val_dataloader: Validation DataLoader.\n","    :param optimizer: Optimizer.\n","    :param criterion: Loss function.\n","    :param num_epochs: Number of epochs.\n","    :param model_name: Name of the model (for display in results).\n","    :param device: Device to use for training ('cuda' or 'cpu').\n","    \"\"\"\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    results = {\n","        'experiment_name': experiment_name,\n","        'model_state_dict': None,\n","        'results_emotions': [],\n","        'results_sentiments': []\n","    }\n","\n","    for epoch in range(num_epochs):\n","        train_losses, train_metrics = train_one_epoch(\n","            model, train_loader, optimizer, criterions, device=device)\n","        val_losses, val_metrics = validate_one_epoch(\n","            model, val_loader, criterions, device=device)\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","        for task in ['emotion', 'sentiment']:\n","            print(f\"\\t{task.capitalize()} Train Loss: {\n","                  train_losses[task]:.4f}\")\n","            print(f\"\\t{task.capitalize()} Val Loss:   {val_losses[task]:.4f}\")\n","            for modality in ['fused']:\n","                train_acc = train_metrics[task][modality]['balanced_acc'] * 100\n","                val_acc = val_metrics[task][modality]['balanced_acc'] * 100\n","                train_f1 = train_metrics[task][modality]['macro_f1'] * 100\n","                val_f1 = val_metrics[task][modality]['macro_f1'] * 100\n","                print(f\"\\t\\t{modality.capitalize()} Train Balanced Acc: {\n","                      train_acc:.2f}%, Train F1 (macro): {train_f1:.2f}%\")\n","                print(f\"\\t\\t{modality.capitalize()} Val Balanced Acc:   {\n","                      val_acc:.2f}%, Val F1 (macro):   {val_f1:.2f}%\")\n","                print('\\n')\n","        print(\"---------------------------------------------------------------\\n\")\n","\n","        # Save results for both tasks\n","        results['results_emotions'].append({\n","            'epoch': epoch + 1,\n","            'train_loss': train_losses['emotion'],\n","            'val_loss': val_losses['emotion'],\n","            'train_metrics': train_metrics['emotion'],\n","            'val_metrics': val_metrics['emotion']\n","        })\n","        results['results_sentiments'].append({\n","            'epoch': epoch + 1,\n","            'train_loss': train_losses['sentiment'],\n","            'val_loss': val_losses['sentiment'],\n","            'train_metrics': train_metrics['sentiment'],\n","            'val_metrics': val_metrics['sentiment']\n","        })\n","\n","    # Save model state\n","    results['model_state_dict'] = model.state_dict()\n","    results['optimizer'] = optimizer\n","\n","    # save all the results as pkl file\n","    results_joblib_path = os.path.join(\n","        save_dir, f\"{experiment_name}_results.pkl\")\n","    joblib.dump(results, results_joblib_path)\n","\n","    # display the summary of the training as a Datframe (a table per task)\n","    # print('\\n\\n SUMMARY OF THE RESULTS\\n')\n","    # for task in ['emotions', 'sentiments']:\n","    #     print(f'Results for the {task} task:')\n","    #     display(task_result_to_table(results[f'results_{task}_bsize{train_dataloader.batch_size}']))\n","    # print(f\"Results saved as Joblib file: {results_joblib_path}.\")\n","\n","    # Display final results in a table\n","    return model, results"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training model...\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/260 [00:00<?, ?it/s]/tmp/ipykernel_92225/2608071565.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  fbank_tensors = [torch.tensor(fbank) for fbank in conv[\"audio_list\"]]\n","                                                 "]},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 6, 80, 3000])\n"]},{"name":"stderr","output_type":"stream","text":["\r"]},{"ename":"ValueError","evalue":"Only mono-channel audio is supported for input to WhisperFeatureExtractor {\n  \"chunk_length\": 30,\n  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n  \"feature_size\": 80,\n  \"hop_length\": 160,\n  \"n_fft\": 400,\n  \"n_samples\": 480000,\n  \"nb_max_frames\": 3000,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"processor_class\": \"WhisperProcessor\",\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 16000\n}\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mmeld_collate_fn)\n\u001b[1;32m      4\u001b[0m dev_loader \u001b[38;5;241m=\u001b[39m DataLoader(dev_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mmeld_collate_fn)\n\u001b[0;32m----> 6\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./saved_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[74], line 259\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, optimizer, criterions, num_epochs, experiment_name, device, save_dir)\u001b[0m\n\u001b[1;32m    251\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m'\u001b[39m: experiment_name,\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_emotions\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_sentiments\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[1;32m    256\u001b[0m }\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 259\u001b[0m     train_losses, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     val_losses, val_metrics \u001b[38;5;241m=\u001b[39m validate_one_epoch(\n\u001b[1;32m    262\u001b[0m         model, val_loader, criterions, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[74], line 42\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterions, emotion_reg, sentiment_reg, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m audio_array \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\n\u001b[1;32m     37\u001b[0m     audio_list, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# audio_array = audio_array.unsqueeze(1)  # adjust dimensions as needed\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m emotion_logits, sentiment_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# If outputs have shape (1, seq_len, num_classes), squeeze the batch dimension\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(emotion_logits\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n","File \u001b[0;32m~/mambaforge/envs/muse/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/mambaforge/envs/muse/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","Cell \u001b[0;32mIn[70], line 40\u001b[0m, in \u001b[0;36mMultimodalClassifierWithLSTM.forward\u001b[0;34m(self, audio_dialog_utts, text_dialog_utts)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_dialog_utts, text_dialog_utts):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_emb:\n\u001b[0;32m---> 40\u001b[0m         audio_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_dialog_utts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# audio_emb = F.adaptive_avg_pool2d(audio_emb, 1)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         audio_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(audio_emb, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m~/mambaforge/envs/muse/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/mambaforge/envs/muse/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","Cell \u001b[0;32mIn[63], line 91\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[0;34m(self, audios)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03mForward pass to process audio and extract embeddings.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m:param audios: Tensor of raw audio waveforms (shape: [batch_size, num_samples]).\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m:return: Mean-pooled audio embeddings (shape: [batch_size, hidden_size]).\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Preprocess the audio if required\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudios\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Move inputs to the same device as the model\u001b[39;00m\n\u001b[1;32m     94\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n","Cell \u001b[0;32mIn[63], line 63\u001b[0m, in \u001b[0;36mAudioEncoder.preprocess_audio\u001b[0;34m(self, audios, sampling_rate)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(audios\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# For models like Whisper that require log-Mel spectrograms\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudios\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Convert PyTorch tensor to NumPy array\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# For models like Wav2Vec2 that directly process raw waveforms\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[1;32m     72\u001b[0m         audios\u001b[38;5;241m.\u001b[39mcpu(),  \u001b[38;5;66;03m# Raw tensor (no conversion needed)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate,\n\u001b[1;32m     74\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     )\n","File \u001b[0;32m~/mambaforge/envs/muse/lib/python3.12/site-packages/transformers/models/whisper/feature_extraction_whisper.py:262\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor.__call__\u001b[0;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, device, return_token_timestamps, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m is_batched_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(raw_speech, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(raw_speech\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batched_numpy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(raw_speech\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly mono-channel audio is supported for input to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m is_batched_numpy \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(raw_speech, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(raw_speech[\u001b[38;5;241m0\u001b[39m], (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)))\n\u001b[1;32m    265\u001b[0m )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n","\u001b[0;31mValueError\u001b[0m: Only mono-channel audio is supported for input to WhisperFeatureExtractor {\n  \"chunk_length\": 30,\n  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n  \"feature_size\": 80,\n  \"hop_length\": 160,\n  \"n_fft\": 400,\n  \"n_samples\": 480000,\n  \"nb_max_frames\": 3000,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"processor_class\": \"WhisperProcessor\",\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 16000\n}\n"]}],"source":["print(\"Training model...\")\n","\n","train_loader = DataLoader(train_set, batch_size=4, shuffle=True, collate_fn=meld_collate_fn)\n","dev_loader = DataLoader(dev_set, batch_size=4, shuffle=False, collate_fn=meld_collate_fn)\n","\n","model, results = train_and_validate(\n","    model, train_loader, dev_loader, \n","    optimizer, criterions, num_epochs, \n","    experiment_name=experiment_name, \n","    device=device, save_dir='./saved_results'\n",")\n","print(\"Training complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6413518,"sourceId":10356240,"sourceType":"datasetVersion"},{"datasetId":6415331,"sourceId":10358898,"sourceType":"datasetVersion"},{"datasetId":6434748,"sourceId":10386819,"sourceType":"datasetVersion"}],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"muse","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"}},"nbformat":4,"nbformat_minor":4}
